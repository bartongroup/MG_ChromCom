---
title: "Chromosome compaction - simple 3-state model"
author: "Marek Gierlinski"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    number_sections: true
---

Collaborators: Tomo Tanaka, John Eykelenboom.

[Cartoon project explanation](http://www.compbio.dundee.ac.uk/user/mgierlinski/chromcom/doc/Cartoon.pdf)

[shiny app](https://shiny.compbio.dundee.ac.uk/marek_chromcom/param_tuner/)

```{r setup, echo=FALSE}
source("../R/setup.R")
source("../R/lib.R")

dataDir <- paste0(projectDir, "data/")
fitDir <- paste0(projectDir, "fits/")
figDir <- paste0(projectDir, "figures/")
bootDir <- paste0(projectDir, "bootstrap/")
tabDir <- paste0(projectDir, "tables/")
pdfDir <- paste0(projectDir, "pdf/")

for(dir in c(fitDir, figDir, bootDir, tabDir, pdfDir)) {
  if(!dir.exists(dir)) dir.create(dir)
}
```

```{r options, echo=FALSE}
opts_chunk$set(
  external = TRUE,
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE
)
options(width = 1000)
options(knitr.table.format = "html") 
```

# Data

```{r read_experimental_data}
echr <- lapply(paste0(dataDir, dataSet, ".csv"), experimentalData)
names(echr) <- dataSet
```

In our data we distinguish four states, marked by colour: blue, brown, pink, red. For the purpose of the model we merge blue and brown together into one state. Here is an example from an experiment with `r nrow(echr$scramble$cells)` cells.

```{r fig_cells, fig.width=5, fig.height=4}
plotCells(echr$scramble) 
```

The next figure shows the proportions of each colour as a function of time. The curves are smoothed with a running mean over the window of 5 time points.

```{r fig_timeline, fig.width=5, fig.height=4}
plotTimelines(echr$scramble, smooth=TRUE, k=5) 
```


# Model

The model consists of three states and a set of rules.

## States

* blue/brown (B)
* pink (P)
* red (R)
 
## Rules

- Simulation is carried out at a discrete time step of 1 min
- There is a fixed time for nuclear envelope breakdown, $t_0 = 0$; $t_0$ can be changed if necessary
- Time $t_1$ is a random variable with exponential distribution with time scale $\tau_1$
- Time $\Delta t_2$ is a random variable with exponential distribution with time scale $\tau_2$
- Time $\Delta t_3$ is a random variable with exponential distribution with time scale $\tau_3$
- The cell is in state B before time $t_0-t_1$.
- B$\rightarrow$P occurs after $t_0-t_1$ with rate $k_1$
- P$\rightarrow$R occurs after $t_0-t_1 + \Delta t_2$ with rate $k_2$*
- P$\rightarrow$B occurs after $t_0-t_1 + \Delta t_3$ with rate $k_3$

\* A switch, $t_{2, ref}$ was introduced to the model, selecting the reference for time $t_2$. In the default position ($t_{2, ref} = 1$) the B$\rightarrow$P activation time is $t_0-t_1 + \Delta t_2$. When the switch is set to $t_{2, ref} = 0$, the activation time is $t_0 + \Delta t_2$, that is, it can only occur after the nuclear envelope breakdown. The idea of the switch was to separate pink and red curves, as observed in some data sets.

I use a Markov chain approach. The next state is generated from the current state based on rules outlined above. The rates, $k$, are converted into probabilities over a given time step $\Delta t$ as $Pr = 1 - e^{1 - k\Delta t}$. The transition times, $t_1$, $t_2 = \Delta t_2$ and $t_3 = \Delta t_3$, are generated before the simulation starts for a given cell.

The cell timeline is repeated for $n_{\rm cell}$ times and then the colour proportions are found at each time point.

Note: though we experimented with varying $t_0$ and a possibility of a "reversed" P$\rightarrow$B transition, they are not discussed in the paper. Also, we don't discuss $t_{2, ref}$ switch.

## Example

Here is an example of the model.

```{r model_example_t1, fig.width=4, fig.height=3}
pars <- c3pars(
  t0 = 0,
  tau1 = 10,
  tau2 = 10,
  k1 = 0.05,
  k2 = 0.04,
  t2ref = 1
)
chr <- ChromCom3(pars)
chr <- generateCells(chr, ncells=3000)
plotTimelines(chr, withpars=TRUE)
```

And here is an example of the the model where P$\rightarrow$R transition can occur only after $t_0 = 0$.

```{r model_example_t0, fig.width=4, fig.height=3}
pars <- c3pars(
  t0 = -10,
  tau1 = 10,
  tau2 = 10,
  k1 = 0.06,
  k2 = 0.03,
  t2ref = 0
)
chr <- ChromCom3(pars)
chr <- generateCells(chr, ncells=3000)
plotTimelines(chr, withpars=TRUE)
```


## Parameter tuner

There is a [shiny app](https://shiny.compbio.dundee.ac.uk/marek_chromcom/param_tuner/) that allows tuning parameters in search for the best solution.

# Fitting model to the data

I attempted fitting model to our data. It is not an easy task, as the model is stochastic. After some testing I found that modified BFGS (a quasi-Newton method, also known as a variable metric algorithm, by Broyden, Fletcher, Goldfarb and Shanno, 1970) which allows box constraints (Byrd et al. 1995), gives reasonable results. Again, due to stochasticity of the model, this algorithm often finds false local minima. I run it several times to find the best minimum. It is a crude and time-consuming method, but gives results better than manual tuning.

Fitting is constrained to time points between -50 and 30 min. The minimized quantity is

${\rm rms} = \sqrt{\sum_{c \in \{B,P,R\}} \sum_i (O_{c,i} - E_{c,i})^2}$

that is, the square root of the sum of squared residuals over all time points and all colours.

## Default model

First, we fit the model with $\tau_1$, $\tau_2$, $k_1$ and $k_2$ free. To improve the chances of finding the correct global minimum, I run the fit with 10,000 cells and 30 tries.

### Scramble

We suspect that $t_0$ might not be exactly zero. Hence I fit the model with $t_0 = 0, -5, -10, -15$ minutes.

Here (and henceforward) I quote the `snakemake` rule (in `Snakefile` file) to create these results:

```
rule fit_scramble_t0
```

Let's read and plot the results.

```{r read_scramble_t0}
neb <- c(0, -5, -10, -15)
chr.scr <- lapply(neb, function(t0) {
  name <- paste0("fitt_scramble_t.", -t0, "_tau1_k1_k2_tau2")
  chr <- readRDS(paste0(fitDir, name, ".rds"))
})
```

```{r fig_scramble_t0, fig.width=5, fig.height=14}
P <- lapply(chr.scr, function(chr)  plotTimelines(chr, expdata = echr$scramble, withpars=TRUE))
grid.arrange(grobs=P, ncol=1) 
```

I think the issue here is not $t_0$ but the fact that the red curve growths is too fast for the model to fully account for it.

### All conditions

```
rule fit_all
```

```{r plot_all_conditions_function}
plotAllConditions <- function(suffix) {
  sets <- dataSet
  chr <- lapply(sets, function(set) {
    fname <- paste0(fitDir, "fits_", set, "_", suffix, ".rds")
    if(file.exists(fname)) {
      readRDS(fname)
    }
  })
  names(chr) <- sets
  P <- list()
  for(set in sets) {
    if(!is.null(chr[[set]])) {
      P[[set]] <- plotTimelines(chr[[set]], expdata = echr[[set]], withpars=TRUE, title=set, title.size=9)
    }
  }
  P
}
```


Here are fit results for all conditions with $t_0$ fixed at zero and four parameters free: $\tau_1$, $\tau_2$, $k_1$ and $k_2$. The RMS is computed over (-50, 30) min range (though the model is shown on the entire range).

```{r fig_all_conditions, fig.width=12, fig.height=20}
P <- plotAllConditions("ref1_t0_tau1_k1_k2_tau2") 
grid.arrange(grobs=P, ncol=2) 
```

## Three parameters, $\tau_2$ fixed at 8 min

We notice (see later) that $\tau_2$ is poorly constrained and similar in all conditions. We fix it at 8 min and fit the model with three free parameters.

```
rule fit_all_3par
```


```{r fig_all_conditions_3par, fig.width=12, fig.height=20}
P <- plotAllConditions("ref1_t0_tau1_k1_k2")
grid.arrange(grobs=P, ncol=2)
```


## RAD21

RAD21 looks problematic. We want to see if it works with higher $\tau_2$. Below, there is a comparison between the default 3-parameter fit ($\tau_2$ fixed at 8 min) and a fit where $\tau_2$ was fixed at 15 min.

```
rule fit_RAD21_t15
```

```{r RAD21_tau8_tau15, fig.width=6, fig.height=4}
rad21.tau8 <- readRDS(paste0(fitDir, "fits_RAD21_ref1_t0_tau1_k1_k2.rds"))
rad21.tau15 <- readRDS(paste0(fitDir, "fits_RAD21_tau2_15_ref1_t0_tau1_k1_k2.rds"))
plotTimelines(rad21.tau8, expdata = echr$RAD21, withpars=TRUE, title="RAD21 tau2=8", title.size=9)
plotTimelines(rad21.tau15, expdata = echr$RAD21, withpars=TRUE, title="RAD21 tau2=15", title.size=9)
```

As we can see the model cannot reproduce RAD21 data because of the excess of pink state at early times. This is consistent with cohesin not been properly established due to RAD21 removal. This data set is beyond the scope of our model.

## Switch: P$\rightarrow$R only after $t_0$

Here are results from a modified model, where P$\rightarrow$R transition can happen only after $t_0$. By default  $t_0 = 0$ and the results are shown in the left panels below. As you can see, the red curve in the model lags behind the red curve in the data. Hence, I fitted the data again, but fixing $t_0 = -10$ min this time (right panels). This aligns the red curves a little better.

We can also see that the pink curve is more peaked, as opposed to the smooth rise and decay in the default model.

```
rule fit_all_ref0
rule fit_all_ref0_t10
```

```{r fig_all_conditions_switch, fig.width=12, fig.height=28}
P1 <- plotAllConditions("ref0_t0_tau1_k1_k2_tau2")
P2 <- plotAllConditions("ref0_t10_tau1_k1_k2_tau2")
P <- c(rbind(P1, P2))  # interleave lists (beautiful trick!)
grid.arrange(grobs=P, ncol=2)
```

## Confidence intervals on fit parameters

It is not easy to find confidence intervals in fit parameters when not only the model is non-analytic, but also stochastic. The only feasible approach it bootstrap, which is computationally expensive. Here I give it a try.

The bootstrap is performed by sampling with replacement from the data and fitting each sample. Then, we look at the distribution of bootstrapped fit parameters.

```{r bootstrap_functions}
# convert original tau in bootstrap results into half-lifes
# replaces tau_1, tau_2, ... with HL_1, HL_2
convertHalfLife <- function(df) {
  cols <- grep("tau", names(df))
  df[, cols] <- log(2) * df[, cols]
  names(df) <- gsub("tau", "HL", names(df))
  df
}

# read bootstrap data created by rule boot_all in the snakefile
# you need to specify the total number of batches, nbatch
# halflife is a logical to indicate conversion of taus into HLs
readBootstrap <- function(set, nbatch, root, halflife) {
  dat <- NULL
  for(batch in 1:nbatch) {
    file <- paste0(bootDir, root, "_", set, "_", batch, ".pars")
    if(file.exists(file)) {
      info <- file.info(file)
      if(info$size > 0) {
        df <- read.table(file, header = TRUE, sep="\t")
        if(halflife) {
          df <- convertHalfLife(df)
        }
        dat <- rbind(dat, df)
      } else {
        warning(paste("file", file, "is empty."))
      }
    } else {
      warning(paste("File", file, "does not exist."))
    }
  }
  dat
}


# read boootstraps for all sets, return a list of data frames
# this is a simple wrapper around 'readBootstrap'
readBootstraps <- function(sets, nboot=300, root="boot", halflife=TRUE) {
  boot <- lapply(sets, function(set) {
    readBootstrap(set, nboot, root, halflife)
  })
  names(boot) <- sets
  boot
}


# find median, 95% CI and 25-27% quantiles
# dat is a data frame with bootstrap results for one set
bootParamStats <- function(dat, pars=c("HL1", "HL2", "k1", "k2")) {
  df <- NULL
  for(par in pars) {
    q <- quantile(dat[[par]], probs=c(0.025, 0.25, 0.5, 0.75, 0.975))
    row <- data.frame(par=par, median=q[3], lo95=q[1], up95=q[5], p25=q[2], p75=q[4])
    df <- rbind(df, row)
  }
  df
}


# plot histograms of bootstrap data
# dat is a data frame with bootstrap results for one set
bootParamPlot <- function(dat, title="", pars=c("HL1", "HL2", "k1", "k2")) {
  # dummy data for x-limits
  vars <- c("HL1", "HL2", "k1", "k2")
  dummy <- data.frame(
    variable = c(vars, vars),
    value = c(3, 3, 0, 0, 20, 20, 0.2, 0.2)
  )
  dummy <- dummy[dummy$variable %in% pars,]
  
  m <- reshape2::melt(dat, measure.vars=pars)
  ggplot(m, aes(value)) +
    geom_blank(data=dummy) +
    geom_histogram(bins=50) +
    facet_wrap(~variable, scales="free", nrow=1) +
    labs(title=title)
}


# create a data frame with bootstrap best-fitting parameters (medians)
# boots - a list of data frames with bootstrap results for all sets
# sets - a list of sets to use
# pars - a vector of parameter names to use
bootstrapParamsTab <- function(boot, sets=NULL, pars=c("HL1", "HL2", "k1", "k2"), format="%.3g") {
  if(is.null(sets)) sets <- names(boot)
  P <- lapply(sets, function(set) {
    bp <- bootParamStats(boot[[set]], pars)
    sprintf(format, bp$median)
  })
  names(P) <- sets
  df <- plyr::ldply(P, .id="set")
  colnames(df) <- c("set", pars)
  df
}


# create a data frame with bootstrap best-fitting parameters, 95% CIs and quartiles
# boots - a list of data frames with bootstrap results for all sets
# sets - a list of sets to use
# pars - a vector of parameter names to use
bootstrapParamsTabCI <- function(boot, sets=NULL, pars=c("HL1", "HL2", "k1", "k2"), format="%.3g") {
  if(is.null(sets)) sets <- names(boot)
  P <- lapply(sets, function(set) {
    bp <- bootParamStats(boot[[set]], pars)
    c(
      sprintf(format, bp$median),
      sprintf(format, bp$lo95),
      sprintf(format, bp$up95),
      sprintf(format, bp$p25),
      sprintf(format, bp$p75)
    )
  })
  names(P) <- sets
  df <- plyr::ldply(P, .id="set")
  colnames(df) <- c("set", paste0(pars, "_med"), paste0(pars, "_lo95"), paste0(pars, "_up95"), paste0(pars, "_p25"), paste0(pars, "_p75"))
  df
}


# plot bootstrap distributions for all sets
# boots - a list of data frames with bootstrap results for all sets
# sets - a list of sets to use
# pars - a vector of parameter names to use
plotBootstrapDistributions <- function(boot, sets=NULL, pars=c("HL1", "HL2", "k1", "k2")) {
  if(is.null(sets)) sets <- names(boot)
  P <- lapply(sets, function(set) {
    bootParamPlot(boot[[set]], set, pars)
  })
  grid.arrange(grobs=P, ncol=1) 
}


# plot correlation of selected bootstrap parameters
# boots - a list of data frames with bootstrap results for all sets
# sets - a list of sets to use
# pars - a vector of two parameter names to use
plotBootstrapParamCor <- function(boot, sets=NULL, pars=c("HL2", "k2")) {
  if(is.null(sets)) sets <- names(boot)
  md <- plyr::ldply(boot, .id="set")
  md <- md[,c("set", pars)]
  
  ggplot(md, aes_string(x=pars[1], y=pars[2])) +
    geom_point(shape=1) +
    geom_smooth(method="loess", se=FALSE, colour="red") +
    facet_wrap(~set)
}


# plot boostrap best-fitting values with 95% CIs
# boots - a list of data frames with bootstrap results for all sets
# sets - a list of sets to use
# pars - a vector of parameter names to use
plotBootstrapCI <- function(boot, sets=NULL, pars=c("HL1", "HL2", "k1", "k2")) {
  if(is.null(sets)) sets <- names(boot)
  P <- lapply(sets, function(set) {
    bootParamStats(boot[[set]], pars)
  })
  names(P) <- sets
  df <- plyr::ldply(P, .id="set")
  df$set <- as.factor(df$set)

  ggplot(df, aes(set, median)) +
    geom_errorbar(aes(x=set, ymin=lo95, ymax=up95), width=0.4) +
    geom_point() +
    facet_wrap(~par, scales="free_y", nrow=1) +
    theme(axis.text.x = element_text(angle = 90, vjust=0.5, hjust=1)) +
    labs(x="Set", y="Fit parameter value") 
}


# plot bootstrap box plots for all data
# boots - a list of data frames with bootstrap results for all sets
# sets - a list of sets to use
# pars - a vector of parameter names to use
plotBootstrapBox <- function(boot, pars=c("HL1", "HL2", "k1", "k2")) {
  md <- plyr::ldply(boot, .id="set")
  md <- md[,c("set", pars)]
  m <- reshape2::melt(md, id.var="set")

  ggplot(m, aes(set, value)) +
    geom_boxplot(outlier.size = 0.1, outlier.colour="grey", position=position_dodge(width=0.8)) +
    facet_wrap(~variable, scales="free_y", nrow=1) +
    theme(axis.text.x = element_text(angle = 90, vjust=0.5, hjust=1)) +
    labs(x="Set", y="Fit parameter value")
}

```

### Bootstraps (4 parameters)

Running bootstraps on the cluster. This is quite slow!

```
rule boot_all
```

Once all is done, we read the results for selected sets.

```{r bootstrap_setup}
nboot <- 300
```

```{r read_bootstraps}
sets <- c("untreated", "scramble", "NCAPD2", "NCAPD3", "SMC2", "RAD21", "WAPL48", "TT108")
boot <- readBootstraps(sets, nboot=nboot, root="boot", halflife=TRUE) 
```

Here is the distribution of bootstrap result. Each bootstrap gives a set of fit parameters. Figures below show the distribution of fit parameters across all bootstraps.

```{r bootstrap_fit_params, fig.width=8, fig.height=14}
plotBootstrapDistributions(boot)
```

And here are fit parameters and their 95% confidence intervals:

```{r bootstrap_fit_params_CI, fig.width=6, fig.height=3}
plotBootstrapCI(boot)
```

These are box plots with whiskers encompassing 90% of data:

```{r bootstrap_fit_params_box, fig.width=6, fig.height=3}
plotBootstrapBox(boot)
```

In this table we show the median, lower and upper 95% CI and the 25th and 75th percentiles of each parameter:

```{r bootstrap_fit_params_median}
df <- bootstrapParamsTabCI(boot)
kable(df, format="html", row.names=FALSE) %>% kable_styling("condensed", full_width = FALSE, position="left", font_size=12) %>% scroll_box(width="800 px")
```

```{r bootstrap_pars_link}
file <- "bootstrap_fit_parameters.txt"
write.table(df, file=paste0(tabDir, file), sep="\t", quote=FALSE, row.names=FALSE, col.names=TRUE)
link <- paste0(public_html, "tables", file) 
```

Here is [link](`r link`) to the above table if you need to download it.

The next plot shows the correlation between $\tau_2$ and $k_2$ across bootstraps. The red line is a local regression line (LOESS).

```{r bootstrap_tau2_k2, fig.width=8, fig.height=7}
plotBootstrapParamCor(boot)
```

### Bootstraps (3 parameters)

Here I perform bootstrap analysis with fixed $\tau_2=8$ min.

```
rule boot_all_3par
```


```{r read_bootstraps_3par}
sets <- c("untreated", "scramble", "NCAPD2", "NCAPD3", "SMC2", "RAD21", "WAPL48", "TT108")
boot.3 <- readBootstraps(sets, nboot=nboot, root="boot_3par")
```

Here is the distribution of bootstrap result.

```{r bootstrap_fit_params_3par, fig.width=6, fig.height=14}
plotBootstrapDistributions(boot.3, pars=c("HL1", "k1", "k2")) 
```

And here are fit parameters and their 95% confidence intervals:

```{r bootstrap_fit_params_CI_3par, fig.width=5, fig.height=3}
plotBootstrapCI(boot.3, pars=c("HL1", "k1", "k2"))
```

These are default box plots with whiskers encompassing 90% of data:

```{r bootstrap_fit_params_box_3par, fig.width=5, fig.height=3}
plotBootstrapBox(boot.3, pars=c("HL1", "k1", "k2")) 
```

In this table we show the median value of each parameter:

```{r bootstrap_fit_params_median_3par}
df <- bootstrapParamsTab(boot.3, pars=c("HL1", "k1", "k2"))
kable(df, format="html", row.names=FALSE) %>% kable_styling("condensed", full_width = FALSE, position="left", font_size=12) 
```

# Four-colour plots

Here I create cell-line plots for all data sets using four colours.


```{r four_colour}
echr4 <- lapply(dataSet, function(set) experimentalData(paste0(dataDir, set, ".csv"), map=model.colours.extended))
names(echr4) <- dataSet
P <- lapply(dataSet, function(set) plotCells(echr4[[set]], palette=c("blue", "chocolate4", "pink", "red")) + labs(title=set))
names(P) <- dataSet
```

```{r fig_four_colours, fig.width=10, fig.height=20}
grid.arrange(grobs=P, ncol=2)
```

PDF files:

```{r four_colours_pdf}
df <- NULL
for(set in dataSet) {
  file <- paste0(set, "_4colour.pdf")
  path.file <- paste0(pdfDir, file)
  url <- paste0(public_html, "pdf/", file)
  link <- paste0("[", set, "](", url, ")")
  df <- rbind(df, data.frame(link=link))
  ggsave(path.file, P[[set]], device="pdf")
}
kable(df, format="html", row.names=FALSE) %>% kable_styling("condensed", full_width = FALSE, position="left", font_size=12) 
```

# Brown density

This work is not included in the paper.

```{r colour_density_functions}

# build a colour map
# a list of logical vectors, different lenght each - trims NAs on both ends
colourMap <- function(echr, set, window=c(-300,-20)) {
  ch <- echr[[set]]
  cells <- ch$cells
  sel.win <- ch$time >= window[1] & ch$time <= window[2]
  cells <- cells[, sel.win]
  colnames(cells) <- ch$time[sel.win]
  map <- cells == "N" | cells == "P"
  cmap <- list()
  k <- 1
  for(i in 1:nrow(map)) {
    row <- map[i, ]
    # trim NAs at both ends
    good <- which(!is.na(row))
    if(length(good) > 0) {
      row <- row[min(good):max(good)]
      cmap[[k]] <- row
      k <- k + 1
    }
  }
  cmap
}

# density of colour points in a map
# number per box (not time!)
cmapDensity <- function(cmap) {
  smap <- na.omit(do.call(c, cmap))
  lambda <- sum(smap) / length(smap)
}

# Fill gaps (NAs) with random data
# Use the same density as existing data
# Pure random (Poisson) distribution
cmapFillNAs <- function(cmap) {
  lambda <- cmapDensity(cmap)
  fmap <- lapply(cmap, function(row) {
    nas <- which(is.na(row))
    if(length(nas) > 0) row[nas] <- runif(length(nas)) < lambda
    row
  })
  fmap
}

# Distribution of interarrival times between individual colour points
# We don't really need this, as this is a bit artificial
interarrivalDistribution <- function(cmap) {
  cmap <- cmapFillNAs(cmap)
  dist <- lapply(cmap, function(row) {
    # mark interarrivals
    w <- which(row)
    w <- as.numeric(names(w))  # time points
    if(length(w) > 1) {
      w[2:length(w)] - w[1:(length(w)-1)]   # interarrivals between colours
    }
  })
  dist[sapply(dist, is.null)] <- NULL  # remove NULLs
  do.call(c, dist)
}


# Distribution of durations of either fills (what="fills")
# or gaps between the fills (what="gap")
# in.bins=TRUE : don't use time information, distribution based on data bins only (needed for random map)
durationDistribution <- function(cmap, what="fills", in.bins=FALSE) {
  min.good <- ifelse(what=="fills", 0, 1)  # for gaps we need at least two events
  
  # WARNING: this is a random fill, so will get slightly different
  # results with every call  to this function
  fmap <- cmapFillNAs(cmap)
  
  # find time step
  if(in.bins) {
    time.step <- 1
  } else {
    lens <- sapply(fmap, length)
    first.tp <- min(which(lens > 1))
    t <- as.numeric(names(fmap[[first.tp]]))
    time.step <- t[2] - t[1]
  }
  
  dist <- lapply(fmap, function(row) {
    good <- which(row)
    w <- NULL
    if(length(good) > min.good) {
      # trim FALSE at ends
      row <- row[min(good):max(good)]
      #  intervals
      r <- rle(row)
      if(what == "fills") {
        w <- r$lengths[r$values]
      } else if (what == "gaps") {
        w <- r$lengths[!r$values]
      }
      w <- as.numeric(w * time.step)
    }
    w
  })
  dist[sapply(dist, is.null)] <- NULL  # remove NULLs
  do.call(c, dist)
}

# simple random map of single-point events
randomMap <- function(cmap) {
  P <- cmapDensity(cmap)
  
  rmap <- lapply(cmap, function(row){
    r <- runif(length(row)) < P
    names(r) <- names(row)
    r
  })
}

# random map by redistributing actual events
randomEventMap <- function(cmap, maxiter=1000, verbose=FALSE) {
  cmap <- cmapFillNAs(cmap)
  events <- durationDistribution(cmap, what="fills", in.bins=TRUE)
  track.len <- sapply(cmap, length)
  P <- track.len / sum(track.len)
  
  # start with empty map of the same lengths
  rmap <- lapply(cmap, function(row) {
    r <- rep(FALSE, length(row))
    names(r) <- names(row)
    r
  })
  
  i <- 1
  # start with the longest
  for(event in events[order(-events)]) {
    cnt <- 1
    done <- FALSE
    while(!done) {
      cell <- which(rmultinom(1, 1, P) == 1)
      if(track.len[cell] >= event) {
        max <- track.len[cell] - event + 1
        pos1 <- sample(1:max, size=1)
        pos2 <- pos1 + event - 1
        range <- pos1:pos2

        # extend range, so we have gaps between events
        if(pos1 > 1) pos1 <- pos1 - 1
        if(pos2 < track.len[cell]) pos2 <- pos2 + 1
        state <- rmap[[cell]][pos1:pos2]  # current state of the random map
      } else {
        # if track is shorter than event
        state = 1
      }
      
      if(sum(state) == 0) { # empty
        rmap[[cell]][range] <- TRUE
        done <- TRUE
        if(verbose) cat(paste("  Done. ", i, "Event =", event, "Cell =", cell, "Range =", paste0(range, collapse=":"), "Iter = ", cnt, "\n"))
        i <- i+1
      } else { # try again
        cnt <- cnt + 1
        if(cnt >= maxiter) done <- TRUE
      }
    }
    if(cnt == maxiter) {
      stop("Did not converge.")
    }
  }
  rmap
}

```

```{r chi2_test}
bin.row <- function(x, bins=2) {
  b <- as.vector(sapply(1:(length(x)/bins), function(i) rep(i, bins)))
  as.numeric(tapply(x, b, sum))
}

contingency.row <- function(d, max.count=30, bins=1) {
  if(max.count %% bins > 0) stop("max.count must be divisible by bins.")
  d <- d[d <= max.count]
  d <- tapply(d, d, length)
  x <- rep(0, max.count)
  x[as.numeric(names(d))] <- d
  if(bins > 1) x <- bin.row(x, bins)
  x
}

chi2.simul <- function(dist, rdist, max.count=30, bins=1) {
  x <- contingency.row(dist, max.count, bins)
  p <- contingency.row(rdist, max.count, bins)
  p[p == 0] <- 0.5
  p <- p / sum(p)
  chi <- chisq.test(x, p=p)
}

chi2.data <- function(dist1, dist2, max.count=30, bins=1) {
  x <- contingency.row(dist1, max.count, bins)
  y <- contingency.row(dist2, max.count, bins)
  mx <- t(matrix(c(x, y), ncol=2))
  sel <- which(colSums(mx) > 0)   # chisq.test doesn't like sums of zeros
  mx <- mx[, sel]
  chisq.test(mx)
}
```


```{r plot_interval_distribution_function}
plotIntervalDistribution <- function(echr, set, window, with.theoretical=FALSE, what="interarrival", bins=1) {
  cmap <- colourMap(echr, set, window=window)
  if(what == "interarrival") {
    dist <- interarrivalDistribution(cmap)
    xlab <- "Point-to-point interval (min)"
  } else if(what %in% c("fills", "gaps")) {
    dist <- durationDistribution(cmap, what=what)
    xlab <- ifelse(what == "fills", "Duration of events (min)", "Gap between events (min)")
  } else {
    stop("Unknown what.")
  }
  
  # find density
  lambda <- cmapDensity(cmap)

  # random dist repeated 100 times to smooth
  r <- lapply(1:100, function(i){
    if(what == "interarrival") {
      rmap <- randomMap(cmap)
      rdist <- interarrivalDistribution(rmap)
    } else if(what == "fills") {
      rmap <- randomMap(cmap)
      rdist <- durationDistribution(rmap, what=what)
    } else {
      rmap <- randomEventMap(cmap)
      rdist <- durationDistribution(rmap, what=what)
    }
  })
  rdist <- do.call(c, r)
  
  chi <- chi2.simul(dist, rdist, bins=bins)
  p.value <- chi$p.value
  if(p.value < 1e-16) p.value <- 0

  # expected theoretical distribution
  xx <- 1:60
  yy <- lambda * exp(-lambda * xx)

  timestep <- echr[[set]]$timepars$step
  
  df <- data.frame(dist=dist)
  rf <- data.frame(dist=rdist)
  breaks <- timestep*(seq(60) - 1.5)
  g <- ggplot(df, aes(dist)) +
    geom_histogram(aes(y=..density..), breaks=breaks) +
    geom_histogram(data=rf, aes(x=dist, y=..density..), breaks=breaks, colour=cbPalette[2], fill=NA) +
    xlim(0, 40) +
    labs(x=xlab, y="Density", title=paste(set, sprintf("d=%5.3f p=%.2g", lambda, p.value)))
  if(with.theoretical) g <- g + geom_point(data=data.frame(x=xx, y=yy), aes(x, y), colour=cbPalette[1])
  g
}
```

In this Section we consider the distribution of brown or pink boxes. Hereafter, I call them "brown", for simplicity, but we do take both brown and pink boxes into account.

Note on terminology: I refer to individual boxes in cell tracks as "boxes" or "points". I refer to chains of consecutive boxes of the same type as "events".

## Point-to-point interarrival distributions

Here I calculate the density of brown boxes and the distribution of inter-arrival times between them. If brown points are distributed randomly with density $\lambda$, the intervals between them should follow the exponential distribution with PDF

$f(d) = \lambda e^{-\lambda d}$

where $d$ is the inter-arrival time between consecutive brown points. Because some of the cells have much shorter data tracks, they will bias the inter-arrival distribution towards shorter intervals. The longer distances are missing from these short tracks. To account for this I made a simple model, in which I take cell tracks from the actual data, fill them randomly with brown points and calculate inter-arrival distribution. This is done 100 times to smooth the distribution.

The example below shows the result for scramble. The black bars represent data, the orange points show the predicted theoretical distribution $f(d) = \lambda e^{-\lambda d}$ and the open blue bars show the simulated random distribution. As we can see, the simulated distribution predicts more short intervals with respect to the theoretical distribution, as expected.

I use a window of [-300, -20] min.

```{r brown_interarrival_example, fig.width=5, fig.height=4}
plotIntervalDistribution(echr4, "scramble", c(-300, -20), with.theoretical = TRUE, what="interarrival") 
```

This plot has limited interpretation. Inter-arrival time between individual one-minute boxes assumes that each box is one event. But in reality, brown event can last less or more than one minute. If we see, e.g., four brown boxes next to each other, it is unlikely that this shows four one-minute events. Much more likely, this is one event that lasts four minutes.

## Brown event duration distribution

Instead, we should consider the distribution of full brown events, short and long. Below, I show the distribution of duration of brown events. Before the durations are calculated, I fill the occasional missing data with random brown or blue points, where brown probability is the same as the overall brown density in the data. This means that this figure is slightly random and the size of bars will change from one instance to another. The differences are rather small, though (no more than a few percent of the bar size).

```{r brown_fills_example, fig.width=5, fig.height=4}
plotIntervalDistribution(echr4, "scramble", c(-300, -20), with.theoretical = FALSE, what="fills") 
```

The blue open bars show the expected distribution of duration of events derived from a random distribution of single brown points (noise). It shows that longer events (3 minutes or longer) are over-represented, by comparison with random noise. Brown occurrences cannot be explained by pure noise, they do form longer non-random "events".

## Brown event gap distribution

We can get more insight from the distribution of gaps between the brown events. Here I use the gaps between the events, not taking brown begin/end into account. For example, if "+" is brown/pink and "-" is blue (or red) then this cell track

```
---+--++++---+-
```

shows three brown events of length 1, 4, and 1 and two "gap" events of lengths 2 and 3. The blue sequences at the beginning and at the end are ignored, because we don't know how long they are, we only see part of them.

The figure below shows the distribution of gaps between brows events. The simulated distribution (open bars) is calculated in the following way:

- First, I fill missing data with random brown points with density $\lambda$. Note that this is rather arbitrary and adds mostly one-minute points. On the other hand these missing points are not frequent, so the it doesn't make huge difference.
- I count the brown events of each length
- I generate random data using the same cell track durations and the same brown events, distributed randomly
- The process is repeated 100 times and the mean distribution of randomized data is shown
- From this I find the distribution of gap length between events

If brown events were random, both data and simulated distributions would be similar. The p-value in the title shows the result from a chi-square test comparing data and simulation. The number "d" in each title is the density of brown boxes ($\lambda$).

```{r brown_gaps_example, fig.width=5, fig.height=4}
plotIntervalDistribution(echr4, "scramble", c(-300, -20), with.theoretical = FALSE, what="gaps") 
```

As we can see, there is an excess of short gaps in comparison to a random distribution. This means that brown events tend to cluster together. In particular, the 1-min gaps are more prominent, suggesting that one brown event might trigger another, immediately after it. On the other hand, this is our timing resolution, so, to some extent, it might be an experimental artifact. For example, if we have a long brown event and one box in the middle is misidentified as blue, than it creates an artificial gap of size 1. If this happens a few times, we get the observed pattern.

Below, are results for all conditions. Note: each plot shows a p-value from a chi-square test between the data and the model shown. These p-values are typically very small. For clarity, I replaced each $p < 10^{-16}$ with zero.

```{r interval_distributions, fig.width=10, fig.height=28}
P <- list()
k <- 1
window <- c(-300, -20)
for(nm in names(echr4)) {
  P[[k]] <- plotIntervalDistribution(echr4, nm, window, what="interarrival")
  P[[k+1]] <- plotIntervalDistribution(echr4, nm, window, what="fills")
  P[[k+2]] <- plotIntervalDistribution(echr4, nm, window, what="gaps")
  k <- k + 3
}
grid.arrange(grobs=P, ncol=3)
```

## Distribution comparison

Here I compare event duration and gap duration distribution for each pair of conditions. I use chi-square test and the null hypothesis is that the distribution does not depend on the sample.

```{r brown_distribution_chisq_function}
getDist <- function(chr, condition, window, what) {
  cmap <- colourMap(chr, condition, window=window)
  if(what == "interarrival") {
    dist <- interarrivalDistribution(cmap)
  } else {
    dist <- durationDistribution(cmap, what=what)
  }
}
 
pMap <- function(what) {
  window <- c(-300, -20)
  n <- length(names(echr4))
  mx <- matrix(rep("-", n*n), n)
  colnames(mx) <- names(echr4)
  rownames(mx) <- names(echr4)
  for(i in 1:(n-1)) {
    for(j in (i+1):n) {
      cond1 <- names(echr4)[i]
      cond2 <- names(echr4)[j]
      dist1 <- getDist(echr4, cond1, window, what)
      dist2 <- getDist(echr4, cond2, window, what)
      chi <- chi2.data(dist1, dist2)
      p <- sprintf("%.2g", chi$p.value)
      mx[cond1, cond2] <- p
      mx[cond2, cond1] <- p
    }
  }
  kable(mx, format="html", row.names=TRUE) %>% kable_styling("condensed", full_width = FALSE, position="left", font_size=12)
}
```


### Point-to-point interval

```{r brown_distribution_chisq_interarrivals}
pMap("interarrival")
```

### Event duration

```{r brown_distribution_chisq_fills}
pMap("fills")
```

### Gaps between events

```{r brown_distribution_chisq_gaps}
pMap("gaps")
```

These tables show that there is very little discernible difference between conditions. Only TT103 differs from some other conditions in gap distribution. Otherwise, we have no evidence to reject the null hypothesis (but we cannot accept it either!).

## S and late G2 data

```{r read_G2_S}
patdataSet <- c("S", "G2")
pchr <- lapply(patdataSet, function(set) experimentalData(paste0(dataDir, set, ".csv"), map=model.colours.extended))
names(pchr) <- patdataSet
```

```{r plot_S_G2, fig.width=6, fig.height=4}
plotCells(pchr$S, palette=c("blue", "chocolate4", "pink", "red"))
plotCells(pchr$G2, palette=c("blue", "chocolate4", "pink", "red"))
```


```{r interval_distributions_G2_S, fig.width=10, fig.height=6}
P <- list()
k <- 1
for(nm in names(pchr)) {
  if(nm == "S") {
    window <- c(-300,300)
  } else {
    window <- c(-300, -20)
  }
  P[[k]] <- plotIntervalDistribution(pchr, nm, window, what="interarrival", bins=2)
  P[[k+1]] <- plotIntervalDistribution(pchr, nm, window, what="fills", bins=2)
  P[[k+2]] <- plotIntervalDistribution(pchr, nm, window, what="gaps", bins=2)
  k <- k + 3
}
grid.arrange(grobs=P, ncol=3) 
```


```{r brown_distribution_chisq_SG2_function}
getDistWin <- function(chr, condition, what) {
  if(condition == "S") {
    window <- c(-300, 300)
  } else {
    window <- c(-300, -20)
  }
  cmap <- colourMap(chr, condition, window=window)
  if(what == "interarrival") {
    dist <- interarrivalDistribution(cmap)
  } else {
    dist <- durationDistribution(cmap, what=what)
  }
}

psgMap <- function(what) {
  allchr <- append(echr4, pchr)
  m <- length(names(pchr))
  n <- length(names(allchr))
  mx <- matrix(rep("-", n*m), m)
  colnames(mx) <- c(names(allchr))
  rownames(mx) <- names(pchr)
  for(i in 1:m) {
    for(j in 1:n) {
      cond1 <- names(pchr)[i]
      cond2 <- names(allchr)[j]
      if(cond1 != cond2) {
        dist1 <- getDistWin(pchr, cond1, what)
        dist2 <- getDistWin(allchr, cond2, what)
        chi <- chi2.data(dist1, dist2, bins=2)
        p <- sprintf("%.2g", chi$p.value)
        mx[cond1, cond2] <- p
      }
    }
  }
  kable(mx, format="html", row.names=TRUE) %>% kable_styling("condensed", full_width = FALSE, position="left", font_size=12)
}
```


### Distribution comparison for point-to-point interval

WARNING: S and G2 data are calculated on a 2-min grid, while other sets are based on 1-min grid. To compare them, I bin their *distributions* from a smaller grid. I don't bin raw data, as there is a problem of the reference frame. Binning 1-2, 3-4, 5-6, ... will give different results than 2-3, 4-5, 6-7, ... 

The distributions are easier to compare. Consider a 1-min grid distribution where we have 50 1-min events and 20 2-min events. I bin them together into 50+20=70 events and compare to the number of 2-min events of S or G2 (which contains all events shorter than 2 minutes).

Depending on how the actual brown events are distributed (their true duration and timing) comparing data based on different time grids might introduce unpredictable biases.

```{r brown_distribution_SG2_chisq_interarrival}
psgMap("interarrival")
```


### Distribution comparison for event duration


```{r brown_distribution_SG2_chisq_fills}
psgMap("fills")
```

### Distribution comparison for gaps between events

```{r brown_distribution_SG2_chisq_gaps}
psgMap("gaps")
```


# Figures

```{r figures_link}
link.fig <- paste0(public_html, "figures")
```

All figures are in the [this folder](`r link.fig`).


```{r fig_read_bootstraps}
#sets <- c("untreated", "scramble", "NCAPD2", "NCAPD3", "SMC2", "RAD21", "WAPL48", "TT108")
sets <- c("untreated", "scramble", "NCAPD2", "NCAPD3", "SMC2")
nboot <- 300
boot <- readBootstraps(sets, nboot=nboot, root="boot", halflife=TRUE)
```

```{r fig_bootstraps}
pdf(paste0(figDir, "bootstraps.pdf"), width=8, height=10)
plotBootstrapDistributions(boot) 
dev.off()
```

```{r fig_boxplots}
pdf(paste0(figDir, "boxplots.pdf"), width=5, height=3.5)
plotBootstrapBox(boot)  
dev.off()
```

```{r fig_boot_parameters}
boot.pars <- bootstrapParamsTab(boot, format="%g") 
```


```{r fig_data_model_function}
plotDataModel <- function(set, boot.pars) {
  p <- boot.pars[boot.pars$set == set, -1, drop=TRUE]
  p <- lapply(p, as.numeric)
  pars <- c3pars(
    tau1 = p$HL1 / log(2),
    tau2 = p$HL2 / log(2),
    k1 = p$k1,
    k2 = p$k2
  )
 chr <- ChromCom3(pars)
 chr <- generateCells(chr, ncells=10000)
 plotTimelines(chr, expdata = echr[[set]], withpars=FALSE, title=set)
}
```

```{r fig_data_model}
P <- lapply(sets, function(set) {
  plotDataModel(set, boot.pars)
})
pdf(paste0(figDir, "data_model.pdf"), width=4, height=14)
grid.arrange(grobs=P, ncol=1)
dev.off()
```


```{r fig_four_colour}
trackDir <- paste0(figDir, "cell_tracks/")
if(!dir.exists(trackDir)) dir.create(trackDir)
for(set in dataSet) {
  dat <- experimentalData(paste0(dataDir, set, ".csv"), map=model.colours.extended)
  pdf(paste0(trackDir, set, ".pdf"), width=4, height=4)
  g <- plotCells(dat, palette=c("blue", "chocolate4", "pink", "red")) + labs(title=set)
  grid.arrange(g)
  dev.off()  
}
```
