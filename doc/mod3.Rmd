---
title: "Chromosome compation - simple 3-state model"
author: "Marek Gierlinski"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    number_sections: true
---

Collaborators: Tomo Tanaka, John Eykelenboom.

[shiny app](https://shiny.compbio.dundee.ac.uk/marek_chromcom/param_tuner/)

```{r setup, include=FALSE}
library(knitr)
library(mylib)
library(ggplot2)
library(gridExtra)
library(latex2exp)
library(kableExtra)
library(parallel)

source("../R/lib.R")
binDir <- "../RData"

opts_chunk$set(fig.path='../figure/mpd3-', 
               cache.path='../cache/mod3-', 
               fig.align='center',
               external=TRUE,
               echo=FALSE,
               warning=FALSE,
               message=FALSE,
               cache=TRUE,
               #fig.pos='H',
               fig.width=4,
               fig.height=3
              )
options(width = 1000)
options(knitr.table.format = "html") 

smth <- 5
```

# Data

```{r read_experimental_data}
echr <- lapply(dataFile, experimentalData)
```

In our data we distinguish four states, marked by colour: blue, brown, pink, red. For the purpose of the model we merge blue and brown together into one state. Here is the result of the experiment from `r nrow(echr$scramble$cells)` cells.

```{r fig_cells, fig.width=5}
plotCells(echr$scramble)
```

The next figure shows the proportions of each colour as a function of time. The curves are smoothed with a running mean over the window of `r smth` time points.

```{r fig_timeline, fig.width=5}
plotTimelines(echr$scramble, smooth=TRUE, k=smth)
```


# Model

The model consists of three states and a set of rules.

## States

* blue/brown (B)
* pink (P)
* red (R)
 
## Rules

- Simulation is carried out at a discrete time step of 1 min
- There is a fixed time for nuclear envelope breakdown, $t_0 = 0$; $t_0$ can be changed if necessary
- Time $t_1$ is a random variable with exponential distribution with time scale $\tau_1$
- Time $\Delta t_2$ is a random variable with exponential distribution with time scale $\tau_2$
- Time $\Delta t_3$ is a random variable with exponential distribution with time scale $\tau_3$
- The cell is in state B before time $t_0-t_1$.
- B$\rightarrow$P occurs after $t_0-t_1$ with rate $k_1$
- P$\rightarrow$R occurs after $t_0-t_1 + \Delta t_2$ with rate $k_2$*
- P$\rightarrow$B occurs after $t_0-t_1 + \Delta t_3$ with rate $k_3$

\* A switch, $t_{2, ref}$ was introduced to the model, selecting the reference for time $t_2$. In the default position ($t_{2, ref} = 1$) the B$\rightarrow$P activation time is $t_0-t_1 + \Delta t_2$. When the switch is set to $t_{2, ref} = 0$, the activation time is $t_0 + \Delta t_2$, that is, it can only occur after the nuclear envelope breakdown. The idea of the switch was to separate pink and red curves, as observed in some data sets.

I use a Markov chain approach. The next state is generated from the current state based on rules outlined above. The rates, $k$, are converted into probabilities over a given time step $\Delta t$ as $Pr = 1 - e^{1 - k\Delta t}$. The transition time, $t_1$, is generated before the simulation starts for a given cell.

The cell timeline is repeated for $n_{\rm cell}$ times and then the colour proportions are found at each time point.

## Example

Here is an example of the model.

```{r model_example_t1, fig.width=4, fig.height=3}
pars <- c3pars(
  t0 = 0,
  tau1 = 10,
  tau2 = 10,
  k1 = 0.05,
  k2 = 0.04,
  t2ref = 1
)
chr <- ChromCom3(pars)
chr <- generateCells(chr, ncells=3000)
plotTimelines(chr, withpars=TRUE)
```

And here is an example of the the model where P$\rightarrow$R transition can occur only after $t_0 = 0$.

```{r model_example_t0, fig.width=4, fig.height=3}
pars <- c3pars(
  t0 = -10,
  tau1 = 10,
  tau2 = 10,
  k1 = 0.06,
  k2 = 0.03,
  t2re = 0
)
chr <- ChromCom3(pars)
chr <- generateCells(chr, ncells=3000)
plotTimelines(chr, withpars=TRUE)
```


## Parameter tuner

There is a [shiny app](https://shiny.compbio.dundee.ac.uk/marek_chromcom/param_tuner/) that allows tuning parameters in search for the best solution.

# Fitting model to the data

I attempted fitting model to our data. It is not an easy task, as the model is stochastic. After some testing I found that modified BFGS (a quasi-Newton method, also known as a variable metric algorithm, by Broyden, Fletcher, Goldfarb and Shanno, 1970) which allows box constraints (Byrd et al. 1995), gives reasonable results. Again, due to stochasticity of the model, this algorithm often finds false local minima. I run it several times to find the best minimum. It is a crude and time-consuming method, but gives results better than manual tuning.

Fitting is constrained to time points between -50 and 30 min. The minimized quantity is

${\rm rms} = \sqrt{\sum_{c \in \{B,P,R\}} \sum_i (O_{c,i} - E_{c,i})^2}$

that is, the square root of the sum of squared residuals over all time points and all colours.

## Default model

First, we fit the model with $\tau_1$, $\tau_2$, $k_1$ and $k_2$ free. To improve the chances of finding the correct global minimum, I run the fit with 10,000 cells and 100 tries.

### Scramble

We suspect that $t_0$ might not be exactly zero. Hence I fit the model with $t_0 = 0, -5, -10, -15$ minutes.

```{r scramble_t0, include=FALSE}
pars <- c3pars(
  t0 = -10,
  tau1 = 15.3,
  tau2 = 9,
  k1 = 0.047,
  k2 = 0.064
)
free <- c("tau1", "k1", "k2", "tau2")

neb <- c(0, -5, -10, -15)
chr.scr <- lapply(neb, function(t0) {
  name <- paste0("fitt_scramble_n10000_t.", -t0, "_tau1_k1_k2_tau2")
  cacheData(name, fitChr, echr$scramble, pars, free, ncells=10000, ntry=100, ncores=4, binDir=binDir, cacheonly=TRUE)
})
names(chr.scr) <- as.character(-neb)
```

```{r fig_scramble_t0, fig.width=5}
for(t0 in names(chr.scr)) {
  print(plotTimelines(chr.scr[[t0]], expdata = echr$scramble, withpars=TRUE))
}
```

I think the issue here is not $t_0$ but the fact that the red curve growths is too fast and the model cannot do it.

### All conditions

```{r, eval=FALSE}
# Somehow, the wrong version was used to do fitting and pars$t2ref is a string.
# Need to convert (only once).
for(set in names(dataFile)) {
  name <- paste0("fits_", set, "_ref1_t0_n100000_tau1_k1_k2_tau2")
  chr <- cacheData(name, binDir=binDir, cacheonly=TRUE)
  chr$pars$t2ref <- 1L
  file <- paste0(binDir, "/", name, ".RData")
  obj <- chr
  save(obj, file=file)
}
```

```{r plot_all_conditions_function}
plotAllConditions <- function(suffix) {
  nms <- names(dataFile)
  chr <- lapply(nms, function(set) {
    name <- paste0("fits_", set, "_", suffix)
    if(file.exists(paste0("../RData/", name, ".RData"))) {
      cacheData(name, binDir=binDir, cacheonly=TRUE)
    }
  })
  names(chr) <- nms
  P <- list()
  for(name in nms) {
    if(!is.null(chr[[name]])) {
      P[[name]] <- plotTimelines(chr[[name]], expdata = echr[[name]], withpars=TRUE, title=name, title.size=9)
    }
  }
  P
}
```


Here are fit results for all conditions with $t_0$ fixed at zero and four parameters free: $\tau_1$, $\tau_2$, $k_1$ and $k_2$.


```{r fig_all_conditions, fig.width=12, fig.height=20}
P <- plotAllConditions("ref1_t0_n10000_tau1_k1_k2_tau2")
grid.arrange(grobs=P, ncol=2)
```


## Switch: P$\rightarrow$R only after $t_0$

Here are results from a modified model, where P$\rightarrow$R transition can happen only after $t_0$. By default  $t_0 = 0$ and the results are shown in the left panels below. As you can see, the red curve in the model lags behind the red curve in the data. Hence, I fitted the data again, but fixing $t_0 = -10$ min this time (right panels). This aligns the red curves a little better.

We can also see that the pink curve is more peaked, as opposed to the smooth rise and decay in the default model.

```{r fig_all_conditions_switch, fig.width=12, fig.height=28}
P1 <- plotAllConditions("ref0_t0_n10000_tau1_k1_k2_tau2")
P2 <- plotAllConditions("ref0_t10_n10000_tau1_k1_k2_tau2")
P <- c(rbind(P1, P2))  # interleave lists (beautiful trick!)
grid.arrange(grobs=P, ncol=2)
```

## Confidence intervals on fit parameters

It is not easy to find confidence intervals in fit parameters when not only the model is non-analytic, but also stochastic. The only feasible approach it bootstrap, which is computationally expensive. Here I give it a try. 

```{r bootstrap_functions}
script <- "/cluster/gjb_lab/mgierlinski/projects/chromcomR/R/bootstrap.R"
logDir <- "/cluster/gjb_lab/mgierlinski/projects/chromcomR/log/"
remoteBootDir <- "/home/mgierlinski/projects/chromcomR/bootstrap/"

runBootstrap <- function(set, nbatch, root="boot", ncells=10000, ntry=30, switch=1, t0=0) {
  for(batch in 1:nbatch) {
    args <- paste(set, root, batch, switch, ncells, ntry, t0)
    cmd <- paste("Rscript", script, args)
    qsub(cmd, logDir, name=paste0(root, "_", set, "_", batch), ncor=8)
  }
}

readBootstrap <- function(set, nbatch, root="boot") {
  dat <- NULL
  for(batch in 1:nbatch) {
    file <- paste0(remoteBootDir, root, "_", set, "_", batch, ".pars")
    if(file.exists(file)) {
      df <- read.table(file, header = TRUE, sep="\t")
      dat <- rbind(dat, df)
    } else {
      warning(paste("File", file, "does not exist."))
    }
  }
  dat
}

bootParamStats <- function(dat) {
  df <- NULL
  for(par in c("tau1", "tau2", "k1", "k2")) {
    q <- quantile(dat[[par]], probs=c(0.025, 0.5, 0.975))
    #q <- signif(q, 3)
    #q <- sprintf("%.2g", q)
    row <- data.frame(par=par, median=q[2], lo=q[1], up=q[3])
    df <- rbind(df, row)
  }
  df
}

bootParamPlot <- function(dat, title="") {
  vars <- c("tau1", "tau2", "k1", "k2")
  
  # dummy data for x-limits
  dummy <- data.frame(
    variable = c(vars, vars),
    value = c(7, 5, 0, 0, 30, 30, 0.2, 0.2)
  )
  
  m <- reshape2::melt(dat, measure.vars=vars)
  ggplot(m, aes(value)) +
    geom_histogram(bins=50) +
    facet_wrap(~variable, scales="free", nrow=1) +
    geom_blank(data=dummy) +
    labs(title=title)
}
```

### Bootstraps

Running bootstraps on the cluster. This is quite slow, so I do it bit by bit.

```{r bootstrap_setup}
nboot <- 300
```

```{r bootstrap_run, eval=FALSE}
runBootstrap("scramble", nboot)
runBootstrap("NCAPD2", nboot)
runBootstrap("NCAPD3", nboot)
```

```{r read_bootstraps}
sets <- c("scramble", "NCAPD2", "NCAPD3")
boot <- lapply(sets, function(set) {
  readBootstrap(set, nboot)
})
names(boot) <- sets
```


### Bootstrap results

Here is the distribution of bootstrap result. Each bootstrap gives a set of fit parameters. Figures below show the distribution of fit parameters across all bootstraps.

```{r bootstrap_fit_params, fig.width=8, fig.height=6}
P <- lapply(sets, function(set) {
  bootParamPlot(boot[[set]], set)
})
grid.arrange(grobs=P, ncol=1)
```

And here are fit parameters and their 95% confidence intervals:

```{r bootstrap_fit_params_CI, fig.width=3, fig.height=4}
P <- lapply(sets, function(set) {
  bootParamStats(boot[[set]])
})
names(P) <- sets
df <- plyr::ldply(P, .id="set")
df$set <- as.factor(df$set)


ggplot(df, aes(set, median)) +
  geom_errorbar(aes(x=set, ymin=lo, ymax=up), width=0.4) +
  geom_point() +
  facet_wrap(~par, scales="free_y") +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(x="Set", y="Fit parameter value")
```

# Four-colour plots

Here I create cell-line plots for all data sets using four colours.


```{r four_colour}
echr4 <- lapply(dataFile, function(file) experimentalData(file, map=model.colours.extended))
P <- lapply(names(echr4), function(nm) plotCells(echr4[[nm]], palette=c("blue", "chocolate4", "pink", "red")) + labs(title=nm))
names(P) <- names(echr4)
```

```{r fig_four_colours, fig.width=12, fig.height=28}
grid.arrange(grobs=P, ncol=2) 
```

PDF files:

```{r four_colours_pdf}
public_html <- "http://www.compbio.dundee.ac.uk/user/mgierlinski/chromcom/"
df <- NULL
for(nm in names(P)) {
  file <- paste0(nm, "_4colour.pdf")
  remote.file <- paste0("/home/mgierlinski/projects/chromcom/pdf/", file)
  url <- paste0(public_html, "pdf/", file)
  link <- paste0("[", nm, "](", url, ")")
  df <- rbind(df, data.frame(link=link))
  ggsave(remote.file, P[[nm]], device="pdf")
}
kable(df, format="html", row.names=FALSE) %>% kable_styling("condensed", full_width = FALSE, position="left", font_size=12)
```

# Brown density

```{r colour_density_functions}

# build a colour map
# a list of logical vectors, different lenght each - trims NAs on both ends
colourMap <- function(echr, condition, window=c(-300,-20), colour="N") {
  ch <- echr[[condition]]
  cells <- ch$cells
  sel.win <- ch$time >= window[1] & ch$time <= window[2]
  cells <- cells[, sel.win]
  colnames(cells) <- ch$time[sel.win]
  map <- cells == colour
  cmap <- list()
  k <- 1
  for(i in 1:nrow(map)) {
    row <- map[i, ]
    # trim NAs at both ends
    good <- which(!is.na(row))
    if(length(good) > 0) {
      row <- row[min(good):max(good)]
      cmap[[k]] <- row
      k <- k + 1
    }
  }
  cmap
}

# density of colour points in a map
# number per box (not time!)
cmapDensity <- function(cmap) {
  smap <- na.omit(do.call(c, cmap))
  lambda <- sum(smap) / length(smap)
}

# Fill gaps (NAs) with random data
# Use the same density as existing data
# Pure random (Poisson) distribution
cmapFillNAs <- function(cmap) {
  lambda <- cmapDensity(cmap)
  fmap <- lapply(cmap, function(row) {
    nas <- which(is.na(row))
    if(length(nas) > 0) row[nas] <- runif(length(nas)) < lambda
    row
  })
  fmap
}

# Distribution of interarrival times between individual colour points
# We don't really need this, as this is a bit artificial
interarrivalDistribution <- function(cmap) {
  cmap <- cmapFillNAs(cmap)
  dist <- lapply(cmap, function(row) {
    # mark interarrivals
    w <- which(row)
    w <- as.numeric(names(w))  # time points
    if(length(w) > 1) {
      w[2:length(w)] - w[1:(length(w)-1)]   # interarrivals between colours
    }
  })
  dist[sapply(dist, is.null)] <- NULL  # remove NULLs
  do.call(c, dist)
}


# Distribution of durations of either fills (what="fills")
# or gaps between the fills (what="gap")
# in.bins=TRUE : don't use time information, distribution based on data bins only (needed for random map)
durationDistribution <- function(cmap, what="fills", in.bins=FALSE) {
  min.good <- ifelse(what=="fills", 0, 1)  # for gaps we need at least two events
  
  # WARNING: this is a random fill, so will get slightly different
  # results with every call  to this function
  fmap <- cmapFillNAs(cmap)
  
  # find time step
  if(in.bins) {
    time.step <- 1
  } else {
    lens <- sapply(fmap, length)
    first.tp <- min(which(lens > 1))
    t <- as.numeric(names(fmap[[first.tp]]))
    time.step <- t[2] - t[1]
  }
  
  dist <- lapply(fmap, function(row) {
    good <- which(row)
    w <- NULL
    if(length(good) > min.good) {
      # trim FALSE at ends
      row <- row[min(good):max(good)]
      #  intervals
      r <- rle(row)
      if(what == "fills") {
        w <- r$lengths[r$values]
      } else if (what == "gaps") {
        w <- r$lengths[!r$values]
      }
      w <- as.numeric(w * time.step)
    }
    w
  })
  dist[sapply(dist, is.null)] <- NULL  # remove NULLs
  do.call(c, dist)
}

# simple random map of single-point events
randomMap <- function(cmap) {
  P <- cmapDensity(cmap)
  
  rmap <- lapply(cmap, function(row){
    r <- runif(length(row)) < P
    names(r) <- names(row)
    r
  })
}

# random map by redistributing actual events
randomEventMap <- function(cmap, maxiter=1000, verbose=FALSE) {
  cmap <- cmapFillNAs(cmap)
  events <- durationDistribution(cmap, what="fills", in.bins=TRUE)
  track.len <- sapply(cmap, length)
  P <- track.len / sum(track.len)
  
  # start with empty map of the same lengths
  rmap <- lapply(cmap, function(row) {
    r <- rep(FALSE, length(row))
    names(r) <- names(row)
    r
  })
  
  i <- 1
  for(event in events) {
    cnt <- 1
    done <- FALSE
    while(!done) {
      cell <- which(rmultinom(1, 1, P) == 1)
      if(track.len[cell] >= event) {
        max <- track.len[cell] - event + 1
        pos1 <- sample(1:max, size=1)
        pos2 <- pos1 + event - 1
        range <- pos1:pos2

        # extend range, so we have gaps between events
        if(pos1 > 1) pos1 <- pos1 - 1
        if(pos2 < track.len[cell]) pos2 <- pos2 + 1
        state <- rmap[[cell]][pos1:pos2]  # current state of the random map
      } else {
        # if track is shorter than event
        state = 1
      }
      
      if(sum(state) == 0) { # empty
        rmap[[cell]][range] <- TRUE
        done <- TRUE
        if(verbose) cat(paste("  Done. ", i, "Event =", event, "Cell =", cell, "Range =", paste0(range, collapse=":"), "Iter = ", cnt, "\n"))
        i <- i+1
      } else { # try again
        cnt <- cnt + 1
        if(cnt >= maxiter) done <- TRUE
      }
    }
    if(cnt == maxiter) {
      stop("Did not converge.")
    }
  }
  rmap
}

```

```{r chi2_test}
bin.row <- function(x, bins=2) {
  b <- as.vector(sapply(1:(length(x)/bins), function(i) rep(i, bins)))
  as.numeric(tapply(x, b, sum))
}

contingency.row <- function(d, max.count=30, bins=1) {
  if(max.count %% bins > 0) stop("max.count must be divisible by bins.")
  d <- d[d <= max.count]
  d <- tapply(d, d, length)
  x <- rep(0, max.count)
  x[as.numeric(names(d))] <- d
  if(bins > 1) x <- bin.row(x, bins)
  x
}

chi2.simul <- function(dist, rdist, max.count=30, bins=1) {
  x <- contingency.row(dist, max.count, bins)
  p <- contingency.row(rdist, max.count, bins)
  p[p == 0] <- 0.5
  p <- p / sum(p)
  chi <- chisq.test(x, p=p)
}

chi2.data <- function(dist1, dist2, max.count=30, bins=1) {
  x <- contingency.row(dist1, max.count, bins)
  y <- contingency.row(dist2, max.count, bins)
  mx <- t(matrix(c(x, y), ncol=2))
  sel <- which(colSums(mx) > 0)   # chisq.test doesn't like sums of zeros
  mx <- mx[, sel]
  chisq.test(mx)
}
```


```{r plot_interval_distribution_function}
plotIntervalDistribution <- function(echr, condition, window, with.theoretical=FALSE, what="interarrival", bins=1) {
  cmap <- colourMap(echr, condition, window=window)
  if(what == "interarrival") {
    dist <- interarrivalDistribution(cmap)
    xlab <- "Point-to-point distance (min)"
  } else if(what %in% c("fills", "gaps")) {
    dist <- durationDistribution(cmap, what=what)
    xlab <- ifelse(what == "fills", "Duration of events (min)", "Gap between events (min)")
  } else {
    stop("Unknown what.")
  }
  
  # find density
  lambda <- cmapDensity(cmap)

  # random dist repeated 100 times to smooth
  r <- lapply(1:100, function(i){
    if(what == "interarrival") {
      rmap <- randomMap(cmap)
      rdist <- interarrivalDistribution(rmap)
    } else {
      rmap <- randomEventMap(cmap)
      rdist <- durationDistribution(rmap, what=what)
    }
  })
  rdist <- do.call(c, r)
  
  chi <- chi2.simul(dist, rdist, bins=bins)
  p.value <- chi$p.value

  # expected theoretical distribution
  xx <- 1:60
  yy <- lambda * exp(-lambda * xx)

  timestep <- echr[[condition]]$timepars$step
  
  df <- data.frame(dist=dist)
  rf <- data.frame(dist=rdist)
  breaks <- timestep*(seq(60) - 1.5)
  g <- ggplot(df, aes(dist)) +
    geom_histogram(aes(y=..density..), breaks=breaks) +
    geom_histogram(data=rf, aes(x=dist, y=..density..), breaks=breaks, colour=cbPalette[2], fill=NA) +
    xlim(0, 40) +
    labs(x=xlab, y="Density", title=paste(condition, sprintf("d=%5.3f p=%.2g", lambda, p.value)))
  if(with.theoretical) g <- g + geom_point(data=data.frame(x=xx, y=yy), aes(x, y), colour=cbPalette[1])
  g
}
```

## Point-to-point interarrival distributions

Here I calculate the density of brown boxes and the distribution of inter-arrival times between them. If brown points are distributed randomly with density $\lambda$, the intervals between them should follow the exponential distribution with PDF

$f(d) = \lambda e^{-\lambda d}$

where $d$ is the inter-arrival time between consecutive brown points. Because some of the cells have much shorter data tracks, they will bias the inter-arrival distribution towards shorter intervals. The longer distances are missing from these short tracks. To account for this I made a simple model, in which I take cell tracks from the actual data, fill them randomly with brown points and calculate inter-arrival distribution. This is done 100 times to smooth the distribution.

The example below shows the result for scramble. The black bars represent data, the orange points show the predicted theoretical distribution $f(d) = \lambda e^{-\lambda d}$ and the open blue bars show the simulated random distribution. As we can see, the simulated distribution predicts more short intervals with respect to the theoretical distribution, as expected.

I use a window of [-300, -20] min.

```{r brown_interarrival_example, fig.width=5, fig.height=4}
plotIntervalDistribution(echr4, "scramble", c(-300, -20), with.theoretical = TRUE, what="interarrival")
```

However, this is not what we really want. Inter-arrival time between individual one-minute boxes assumes that each box is one event. But in reality, brown event can last less or more than one minute. If we see, e.g., four brown boxes next to each other, it is unlikely that this shows four one-minute events. Much more likely, this is one event that lasts four minutes. Hence, considering the distribution of times between one-minute boxes and comparing it to a Poisson distribution of random one-minute events does not make much sense.

## Brown event duration and gap distribution

Instead, we should consider the distribution of full brown events, short and long. Below, I show the distribution of duration of brown events.

```{r brown_fills_example, fig.width=5, fig.height=4}
plotIntervalDistribution(echr4, "scramble", c(-300, -20), with.theoretical = FALSE, what="fills") 
```

NOTE: the blue open bars show the simulated data (see details below). There is a slight difference between real and simulated data. This is because for each simulation (which is repeated 100 times) gaps in real data are filled with random data. Hence, the simulated data shows the most likely real data, with imputation.

We can get more insight from the distribution of intervals between the brown events. Here I use the gaps between the events, not taking brown begin/end into account. For example, if "+" is brown and "-" is blue then this cell track

```
---+--++++---+-
```

shows three brown events of length 1, 4, and 1 and two "gap" events of lengths 2 and 3. The blue sequences at the beginning and at the end are ignored, because we don't know how long they are, we only see part of them.

This figure shows the distribution of gaps between brows events. The simulated distribution (open bars) is calculated in the following way:

- I fill the gaps in data with random brown points with density $\lambda$. Note that this is rather arbitrary and adds mostly one-minute points. On the other hand these gaps are not large, so the it doesn't make huge difference.
- I count the brown events of each length
- I generate random data using the same cell track durations and the same brown events, distributed randomly
- The process is repeated 100 times and the mean distribution of randomized data is shown
- This can be applied both to even duration distribution and gap between events distribution

If brown events were random, both data and simulated distributions would be similar. The p-value in the title shows the result from a chi-square test comparing data and simulation. The number "d" in each title is the density of brown boxes ($\lambda$).

```{r brown_gaps_example, fig.width=5, fig.height=4}
plotIntervalDistribution(echr4, "scramble", c(-300, -20), with.theoretical = FALSE, what="gaps")
```

As we can see, there is an excess of short gaps in comparison to a random distribution. This means that brown events tend to cluster together. In particular, the 1-min gaps are more prominent, suggesting that one brown event might trigger another, immediatelly after it. On the other hand, this is our timing resolution, so, to some extent, it might be an experimental artifact. For example, if we have a long brown event and one box in the middle is misidentified as blue, than it creates an artificial gap of size 1. If this happens a few times, we get the observed pattern.

Below, are results for all conditions. Both event duration (left) and gap distribution (right).

```{r interval_distributions, fig.width=7, fig.height=28}
P <- list()
k <- 1
for(nm in names(echr4)) {
  P[[k]] <- plotIntervalDistribution(echr4, nm, c(-300, -20), what="fills")
  P[[k+1]] <- plotIntervalDistribution(echr4, nm, c(-300, -20), what="gaps")
  k <- k + 2
}
grid.arrange(grobs=P, ncol=2)
```

## Distribution comparison

Here I compare event duration and gap duration distribution for each pair of conditions. I use chi-square test and the null hypothesis is that the distribution does not depend on the sample.

```{r brown_distribution_chisq_function}
getDist <- function(chr, condition, window, what) {
  cmap <- colourMap(chr, condition, window=window)
  dist <- durationDistribution(cmap, what=what)
}
 
pMap <- function(what) {
  window <- c(-300, -20)
  n <- length(names(echr4))
  mx <- matrix(rep("-", n*n), n)
  colnames(mx) <- names(echr4)
  rownames(mx) <- names(echr4)
  for(i in 1:(n-1)) {
    for(j in (i+1):n) {
      cond1 <- names(echr4)[i]
      cond2 <- names(echr4)[j]
      dist1 <- getDist(echr4, cond1, window, what)
      dist2 <- getDist(echr4, cond2, window, what)
      chi <- chi2.data(dist1, dist2)
      p <- sprintf("%.2g", chi$p.value)
      mx[cond1, cond2] <- p
      mx[cond2, cond1] <- p
    }
  }
  kable(mx, format="html", row.names=TRUE) %>% kable_styling("condensed", full_width = FALSE, position="left", font_size=12)
}
```

### Distribution comparison for event duration

```{r brown_distribution_chisq_fills}
pMap("fills")
```

### Distribution comparison for gaps between events

```{r brown_distribution_chisq_gaps}
pMap("gaps")
```

These tables show that there is very little discernible difference between conditions. Only TT103 differs from some other conditions in gap distribution. Otherwise, we have no evidence to reject the null hypothesis (but we cannot accept it either!).

## S and late G2 data

```{r read_G2_S}
patDataFile <- list(
  S_phase = "../data/TT104_patterning_S_noncoloured.csv",
  G2_phase = "../data/TT104_patterning_late_G2_noncoloured.csv"
)
pchr <- lapply(patDataFile, function(file) experimentalData(file, map=model.colours.extended))
```

```{r plot_S_G2, fig.width=6, fig.height=4}
plotCells(pchr$S_phase, palette=c("blue", "chocolate4", "pink", "red"))
plotCells(pchr$G2_phase, palette=c("blue", "chocolate4", "pink", "red"))
```


```{r interval_distributions_G2_S, fig.width=7, fig.height=6}
P <- list()
k <- 1
for(nm in names(pchr)) {
  if(nm == "S_phase") {
    window <- c(-300,300)
  } else {
    window <- c(-300, -20)
  }
  P[[k]] <- plotIntervalDistribution(pchr, nm, window, what="fills", bins=2)
  P[[k+1]] <- plotIntervalDistribution(pchr, nm, window, what="gaps", bins=2)
  k <- k + 2
}
grid.arrange(grobs=P, ncol=2)
```


```{r brown_distribution_chisq_SG2_function}
getDistWin <- function(chr, condition, what) {
  if(condition == "S_phase") {
    window <- c(-300, 300)
  } else {
    window <- c(-300, -20)
  }
  cmap <- colourMap(chr, condition, window=window)
  dist <- durationDistribution(cmap, what=what)
}

psgMap <- function(what) {
  allchr <- append(echr4, pchr)
  m <- length(names(pchr))
  n <- length(names(allchr))
  mx <- matrix(rep("-", n*m), m)
  colnames(mx) <- c(names(allchr))
  rownames(mx) <- names(pchr)
  for(i in 1:m) {
    for(j in 1:n) {
      cond1 <- names(pchr)[i]
      cond2 <- names(allchr)[j]
      if(cond1 != cond2) {
        dist1 <- getDistWin(pchr, cond1, what)
        dist2 <- getDistWin(allchr, cond2, what)
        chi <- chi2.data(dist1, dist2, bins=2)
        p <- sprintf("%.2g", chi$p.value)
        mx[cond1, cond2] <- p
      }
    }
  }
  kable(mx, format="html", row.names=TRUE) %>% kable_styling("condensed", full_width = FALSE, position="left", font_size=12)
}
```


### Distribution comparison for event duration

```{r brown_distribution_SG2_chisq_fills}
psgMap("fills")
```

### Distribution comparison for gaps between events

```{r brown_distribution_SG2_chisq_gaps}
psgMap("gaps")
```
